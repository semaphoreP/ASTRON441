{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.ndimage as ndimage\n",
    "import timeit\n",
    "import multiprocessing\n",
    "import astropy.io.fits as fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLAS/LAPACK/ATLAS/MKL\n",
    "\n",
    "This often isn't the parallelism you are looking for, but it is a way to speed up single-threaded code that relies heavily on matrix operations.\n",
    "\n",
    "There are packages that automatically parallelize linear algebra routines like matrix dot product or singular value decomposition. BLAS/LAPACK are the algorithms, and ATLAS, OpenBLAS, or MKL are implementations of them. If you have an Intel processor, you will be using MKL. They are quite useful for when you don't want to write your own parallelization code but want to speed up matrix routines. It requires numpy to be configured for BLAS/MKL properly. Generally if you install numpy with Anaconda, this happens automatically (you should already be benefitting from it), so this is why I recommend installing numpy using `conda install numpy`. These speedups are turned on by default when configured.\n",
    "\n",
    "### Note on when to use\n",
    "\n",
    "However, if you wish to do your own parallelism, make sure to turn this off. Having multiple layers of parallelism will often slow down your code! You can turn off these parallelism features in your `.bashrc` using:\n",
    "\n",
    "```\n",
    "export OPENBLAS_NUM_THREADS=1\n",
    "export MKL_NUM_THREADS=1\n",
    "```\n",
    "\n",
    "### How to check if you have these libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blas_mkl_info:\n",
      "    libraries = ['mkl_rt', 'pthread']\n",
      "    library_dirs = ['/home/jwang/miniconda3/envs/codeastro/lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['/home/jwang/miniconda3/envs/codeastro/include']\n",
      "blas_opt_info:\n",
      "    libraries = ['mkl_rt', 'pthread']\n",
      "    library_dirs = ['/home/jwang/miniconda3/envs/codeastro/lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['/home/jwang/miniconda3/envs/codeastro/include']\n",
      "lapack_mkl_info:\n",
      "    libraries = ['mkl_rt', 'pthread']\n",
      "    library_dirs = ['/home/jwang/miniconda3/envs/codeastro/lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['/home/jwang/miniconda3/envs/codeastro/include']\n",
      "lapack_opt_info:\n",
      "    libraries = ['mkl_rt', 'pthread']\n",
      "    library_dirs = ['/home/jwang/miniconda3/envs/codeastro/lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['/home/jwang/miniconda3/envs/codeastro/include']\n"
     ]
    }
   ],
   "source": [
    "# Check we have BLAS/LAPACK. Notice they are from the MKL library\n",
    "np.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the speed-up from these libraries, we will try some matrix dot product with it turned on and off. In the code block below, we are using the MKL package which works with Intel CPUs. If you do not have an Intel CPU, this command will not do anything. Setting the number of threads this way for MKL is effectively the same as doing it in your `.bashrc` but is not permanent (only affects this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 cores 1.5699356000404805\n",
      "1 core 2.944283500080928\n"
     ]
    }
   ],
   "source": [
    "##### Note, this following demo only works if you can use MKL. \n",
    "import mkl\n",
    "\n",
    "mat = np.random.random((1000, 1500)) # 1000x1500 matrix\n",
    "\n",
    "# Use 4 cores to parallelize matrix operations\n",
    "mkl.set_num_threads(4)\n",
    "print(\"4 cores\", timeit.timeit(\"mat.dot(mat.T)\", setup=\"from __main__ import mat\", number=100))\n",
    "\n",
    "# Disable MKL\n",
    "mkl.set_num_threads(1)\n",
    "print(\"1 core\", timeit.timeit(\"mat.dot(mat.T)\", setup=\"from __main__ import mat\", number=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Multiple Python Instances\n",
    "\n",
    "This might sound very unsophisticated, but it is actually a good choice for many instances. Python parallelism is not the best, and if tasks are completely independent of each other, running each task as a separate Python process and saving the result as a file is a perfectly reasonable (and simple) option. For example, this is great for bulk processing a bunch of files with some data reduction code. There are many options to do this: bash script, GNU Parallel, or, a master python script.\n",
    "\n",
    "## GNU Parallel\n",
    "\n",
    "If you have a script that you want to run multiple times (e.g., on multiple files), you don't need to write Python parallelization. We can use the `parallel` package that can be installed on UNIX systems to handle the parallelization. For example, if we can process a single file (doesn't have to be a file) by running on the command line:\n",
    "\n",
    "    python process.py file_01.fits\n",
    "\n",
    "Then we can run it on `file_01.fits` to `file_99.fits` by running on the command line:\n",
    "\n",
    "    parallel python process.py ::: file_{01..99}.fits\n",
    "\n",
    "And that's it! You don't have to do anything else but make it so your python script can take a filename as input! (Hint: you can use `import sys; sys.argv` to grab command line arguments in your code).\n",
    "\n",
    "## Python Scripting\n",
    "\n",
    "I am not going to show you how to do this in a bash script. Instead, we will focus on using a python script to launch a bunch of python processes because all capabilities of shell scripting (e.g., calling bash commands with the sys module) can be done in Python, and often with much better readability. We will use python to launch a bunch of python processes.\n",
    "\n",
    "We create a function with an argument `index` that tells each proces what chunk of the task to run. We then create a bunch of processes, give them their chunks, and call `start()` to run them. Afterwards, our master process uses `join()` to wait for each process to finish. It is important to always call `join()` at the end to ensure all processes have finished running! If a process has finished immediately, `join()` will immediately return; if a process has not finished, our master process will sleep until the process it is waiting on has finished. \n",
    "\n",
    "### Before we begin with multiprocessing, a few important points:\n",
    "\n",
    " * Only one process can spawn other processes. Your subprocesses cannot spawn their own subprocesses! Generally, this is always a way to program things so that only one process needs to spawn subprocesses\n",
    " * While python is generally OS-agnostic, you may run into OS specific issues with multiprocesses due to the fact it is implemented a bit differently for Linux, Mac, and Windows. \n",
    " * One key difference to remember when developing packages for multiple OSes: on Mac and Windows, any lines with multiprocessing calls need to be wrapped in something like ``if __name__ == \"__main__\":``, otherwise you will get an error mentioning something about `freeze_support()` when trying to run it. This is not required in Jupyter notebooks, so we have merely commented where you would need them here. The details for why this is necessary is related to subprocesses not spawning their own subprocesses and is quite technical, but [here is a related blog post if you want to learn more](https://pythonspeed.com/articles/python-multiprocessing/). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file_0.fits complete\n",
      "Process file_1.fits completeProcess file_2.fits complete\n",
      "\n",
      "Process file_3.fits complete\n",
      "\n",
      "Process file_4.fits completeProcess file_5.fits complete\n",
      "Process file_6.fits complete\n",
      "Process file_7.fits complete\n",
      "Process file_8.fits complete\n",
      "Process file_9.fits complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_data(filename):\n",
    "    \"\"\"\n",
    "    Let's pretend this function does someting\n",
    "\n",
    "    Args:\n",
    "        filename (str): the input file to work on\n",
    "    \"\"\"\n",
    "    # do some data processing\n",
    "    print(\"Process {0} complete\".format(filename))\n",
    "    # could save value to a shared variable or save to a file to be used later\n",
    "\n",
    "process_list = []\n",
    "\n",
    "### If you get errors about freeze_support(): \n",
    "### then following code would need to be wrapped in `if __name__ == \"__main__\":` (both for loops).\n",
    "for i in range(10):\n",
    "    filename = \"file_{0}.fits\".format(i)\n",
    "    p = multiprocessing.Process(target=process_data, args=(filename,))\n",
    "    process_list.append(p)\n",
    "    p.start()\n",
    "\n",
    "for p in process_list:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pools\n",
    "\n",
    "Manually creating threads requires a bunch of upkeep code, which is unnecessary if you are just running over a giant loop. In the spirit of keeping parallelism simple, use a high-level parallelization API provided by your programming language whenever possible! It will save you time and effort (Trust me). For dividing up tasks with a for loop, use Python process `Pools`. Essentially, you can give\n",
    "any number of tasks to a process `Pool` and the processes in the pool will loop through and do each one per your instructions. \n",
    "\n",
    "When in doubt about how to parallelize code, use process pools! They are flexible and can accomodate most use cases. And since they have a standardized interface for how to use them, it will make your code more understadable by others compared to home-brewing your own parallelism. \n",
    "\n",
    "### Matrix dot product example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_pool(mat, index):\n",
    "    # divide up so that we only compute one chunk of the mat.dot(mat.T) matrix\n",
    "    index_start = mat.shape[0] // 10 * index\n",
    "    index_end = mat.shape[0] // 10 * (index + 1)\n",
    "    val  = mat[index_start:index_end].dot(mat.T)\n",
    "    print(\"Job {0} complete\".format(index))\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created job 0\n",
      "Created job 1\n",
      "Created job 2\n",
      "Created job 3\n",
      "Created job 4\n",
      "Created job 5\n",
      "Created job 6\n",
      "Created job 7\n",
      "Created job 8\n",
      "Created job 9\n",
      "Job 0 complete\n",
      "Result 0 (300, 3000)\n",
      "Job 1 complete\n",
      "Result 1 (300, 3000)\n",
      "Job 2 complete\n",
      "Result 2 (300, 3000)\n",
      "Job 3 complete\n",
      "Result 3 (300, 3000)\n",
      "Job 4 complete\n",
      "Result 4 (300, 3000)\n",
      "Job 5 complete\n",
      "Result 5 (300, 3000)\n",
      "Job 6 complete\n",
      "Result 6 (300, 3000)\n",
      "Job 7 complete\n",
      "Result 7 (300, 3000)\n",
      "Job 8 complete\n",
      "Result 8 (300, 3000)\n",
      "Job 9 complete\n",
      "Result 9 (300, 3000)\n"
     ]
    }
   ],
   "source": [
    "### This entire block would need to be wrapped in `if __name__ == \"__main__\":`if you have freeze_support() issues\n",
    "pool = multiprocessing.Pool(processes=8) # creae a pool with 8 worker processes\n",
    "\n",
    "pool_jobs = []\n",
    "for i in range(10):\n",
    "    job = pool.apply_async(matrix_pool, (mat, i))\n",
    "    pool_jobs.append(job)\n",
    "    print(\"Created job {0}\".format(i))\n",
    "\n",
    "for i, job in enumerate(pool_jobs):\n",
    "    result = job.get() \n",
    "    print(\"Result {0}\".format(i), result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Resource Usage\n",
    "\n",
    "Anytime you are testing parallelized code, it is good to monitor your CPU/RAM usage. Monitoring your CPU usage can help you assess if the number of processes being run in parallel is consistent with what you are looking for. Many times, running parallelized code already involves big data sets, and parallelization will use even more memory (you can think of it as trading runtime for memory usage). So have your resource monitor up occasionally when you are developing this code. It is also a great way to debug the code (e.g., identify hanging parallelization that is not finishing). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity: Image Processing\n",
    "\n",
    "Run the following code to generate some fake data and then run the following image processing function on each fake image and save the resulting data. How fast can you process the data when the images are 1000x1000? What is the speedup from 1 core to multiple cores (is it linear?)?\n",
    "\n",
    "### Instructions\n",
    "\n",
    "  * Run the first cell below to generate 25 images of fake data\n",
    "  * Run the second cell below to run the `process_image()` function on a single image without parallelism. Time how long this takes. Let's call this time t0. \n",
    "  * Write parallelization code to parallelize the `process_image()` function to process all 25 images. Time how long it takes to process 25 images. Let's call this tp. \n",
    "  * Compute speedup = (25 * t0) / tp: the speedup factor between doing 25 images in series and parallelizing it. \n",
    "  * (Optional) Try using different number of processes to parallelize the activity. How does the run time change with the number of processes used? How does it relate to the number of cores you have on your computer? Is it linear?\n",
    "\n",
    "### End Product\n",
    "  * Report on Piazza your best speedup factor you got. And specify how many processes you used, and how many cores you have.\n",
    "  * (Optional): make a graph of wall clock time to process 25 images vs number of processes used. Post this on Piazza as well.  \n",
    "\n",
    "### Roles\n",
    "  * Driver: in charge of typing and running the code for this activity\n",
    "  * Navigator: in charge of directing the driver what to code (can be more than one person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, this cell might take a minute to run.\n",
    "\n",
    "def generate_fake_data(filename, dims):\n",
    "    \"\"\"\n",
    "    Generates a fake dataframe with random numbers\n",
    "    \n",
    "    Args:\n",
    "        filename (str): file to save the data to\n",
    "        dims (tuple): (Ny, Nx) pair that species the size of the y and x dimensions\n",
    "    \"\"\"\n",
    "    # some complicated random image generation. Feel free to ignore.\n",
    "    # coordinate system in fourier spae\n",
    "    u,v = np.meshgrid(np.fft.fftfreq(dims[1]), np.fft.fftfreq(dims[0]))\n",
    "    phases = np.random.uniform(0, 2*np.pi, u.shape)\n",
    "    rho = np.sqrt((u*dims[1])**2 + (v*dims[0])**2)\n",
    "    # suppress high frequency by a squared exponential\n",
    "    spectrum = np.exp(-rho**2/(np.max(rho)/50)**2)  * np.exp(1j * phases)\n",
    "    filtered = np.real(np.fft.ifft2(spectrum))\n",
    "\n",
    "    fits.writeto(filename, filtered, overwrite=True)\n",
    "\n",
    "\n",
    "# generate fake data (can choose to save it somethere else if you want)\n",
    "fileformat = os.path.join(\"./\", \"fake_{0}x{1}_{2}.fits\")\n",
    "\n",
    "ny = 1000\n",
    "nx = 1000\n",
    "for i in range(25):\n",
    "    filename = fileformat.format(ny, nx, i)\n",
    "    generate_fake_data(filename, (ny, nx) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(frame, filtersize=50):\n",
    "    \"\"\"\n",
    "    Run a high-pass filter on the data. \n",
    "    Remove the low spatial frequency (i.e., smooth features) in the image\n",
    "\n",
    "    Args:\n",
    "        frame (np.array): a 2-D image to be processed\n",
    "        fitersize (int): the size of the filter. Features smaller than the filtersize will be preserved\n",
    "\n",
    "    Returns:\n",
    "        processed_frame (np.array): a 2-D image after processing\n",
    "    \"\"\"\n",
    "    # run a median filter to smooth the image\n",
    "    frame_smooth = ndimage.median_filter(frame, filtersize)\n",
    "\n",
    "    processed_frame = frame - frame_smooth\n",
    "\n",
    "    return processed_frame\n",
    "\n",
    "# an example of running this on one image\n",
    "with fits.open(fileformat.format(ny, nx, 0)) as hdulist:\n",
    "    data = hdulist[0].data\n",
    "\n",
    "\n",
    "    filt_data = process_image(data)\n",
    "\n",
    "    fig = plt.figure(figsize=(6,3))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax1.imshow(data, cmap=\"inferno\")\n",
    "    ax1.set_title(\"Original\")\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.imshow(filt_data, cmap=\"inferno\")\n",
    "    ax2.set_title(\"Filtered\")\n",
    "\n",
    "#### Activity:\n",
    "# write and time some code that runs this on all 25 images in parallel. How does the performance increase as you increase the number of processes you use?\n",
    "# we recommend you use multiprocessing pool for this task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism Extended Cut\n",
    "\n",
    "There is a lot of talk about in parallelism - we are just scratching the surface. The above concepts _should_ cover the majority of parallelism use cases. However, here are some good things to know.\n",
    "\n",
    "\n",
    "## Do you really need parallelism\n",
    "\n",
    "We already emphasized this above, but parallelism adds complexity. Try avoiding parallelization until it is necessary. If some code takes 10 minutes to run, but you only need to run it once per week, is it worth parallelizing? The programming and upkeep costs may not be worth it. Parallelism also makes it hard to debug: we cannot attach the python debugger on other Python processes meaning they often crash with no error message as to why. GNU Parallel (below) is an alternative to writing more complicated code.\n",
    "\n",
    "## Threads vs Processes\n",
    "Python has two parallelization modules: `multiprocessing` and `multithreading`. What's the difference? Why do we only use `multiprocessing`?\n",
    "\n",
    "Threads and processes are both tasks that your computer CPUs run in parallel. Threads share the same memory, whereas processes do not. Processes have to communicate themselves with shared variables (see below) or via a slower I/O method (writing to files, communicating over websockets). Most languages use threads to parallelize computations because sharing the same memory is very convenient and saves on resources. However, Python has the \"Global Intepreter Lock\" (GIL) that allows only one thread to run at a time (for complicated consistency reasons). Generally, parallelism in astronomy involves computationally intensive tasks so only one of them being able to run at a time would defeat the purpose. That's why you will generally only see multiprocessing in astronomy-related software development.\n",
    "\n",
    "Threads are more frequently seen outside of astronomy, but it is generally useful for \"I/O bound tasks\" as opposed to the \"CPU bound tasks\" that we usually encounter in astronomy. Anytime you have tasks with a lot of sleeping/waiting time such as if you have multiple web API queries (e.g., multiple database queries) or waiting for files to be created, threads are a better choice because they use less memory and they can access all of your variables instead of having to define shared variables.\n",
    "\n",
    "The one notable exception is that `numpy` functions that call C code releases the GIL. This means if your code is dominated by `numpy` matrix operations such as dot product, then you can actually use threads with minimal increase in runtime.\n",
    "\n",
    "## Shared Variables\n",
    "\n",
    "Processes do not by default share memory. For processes to read/write/access the same variables, we have to declare shared memory. Python `multiprocessing` provides nearly all shared memory structure that you should be using: queues and arrays. Queues allow for interprocess communication. Arrays allow for large datasets to be accessed by multiple processes without needing to duplicate them (possibly saving a lot of memory usage). Use these special arrays and queues becasue they are automatically synchronized between processes and do not require synchronization code, which is generally low-level synchronization that we should avoid programming unless we absolutely need it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('codeastro')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "70da8eaca2f829bebd9ae4bfee73e2015b5a57abadfaaa95753587bc60143f91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
